{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color: green;'><center>Question 50 <br> Machine Learning</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q1 What is the differnece between Series & Dataframe.</h3>\n",
    "<p><B>Answer</B></p>\n",
    "Following are the difference between Series and dataframe.\n",
    "  <table>\n",
    "        <tr>\n",
    "            <th>Feature</th>\n",
    "            <th>Series</th>\n",
    "            <th>DataFrame</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Dimensionality</td>\n",
    "            <td>One-dimensional (single column)</td>\n",
    "            <td>Two-dimensional (rows and columns)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Data Type Homogeneity</td>\n",
    "            <td>All elements must have the same data type</td>\n",
    "            <td>Columns can have different data types (int, string, etc.)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Size Mutability</td>\n",
    "            <td>Immutable (size cannot be changed)</td>\n",
    "            <td>Mutable (size can be dynamically changed)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Use Cases</td>\n",
    "            <td>Simple data sequences, univariate analysis</td>\n",
    "            <td>Tabular data, relational data, multivariate analysis</td>\n",
    "        </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q2 Create a database name Travel_Planner in mysql, and create a table name bookings in that which having attributes (user_id INT, flight_id INT, hotel_id INT, activity_id INT, booking_date DATE). Fill with some dummy values. Now you have to read the content as pandas dataframe and display the output.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>flight_id</th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>booking_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3462</td>\n",
       "      <td>465</td>\n",
       "      <td>789</td>\n",
       "      <td>2024-07-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4564</td>\n",
       "      <td>675</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6436</td>\n",
       "      <td>263</td>\n",
       "      <td>465</td>\n",
       "      <td>2024-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3642</td>\n",
       "      <td>783</td>\n",
       "      <td>674</td>\n",
       "      <td>2024-07-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  flight_id  hotel_id  activity_id booking_date\n",
       "0        1       3462       465          789   2024-07-10\n",
       "1        2       4564       675          246   2024-07-12\n",
       "2        3       6436       263          465   2024-07-08\n",
       "3        4       3642       783          674   2024-07-09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example\n",
    "## We already create a database named Travel_Planner and a table named with bookings with dummy values.\n",
    "\n",
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "\n",
    "database=sql.connect('Travel_Planner.db')\n",
    "cursor=database.cursor()\n",
    "\n",
    "query=f\"SELECT * FROM bookings\"\n",
    "df=pd.read_sql_query(query, database)\n",
    "\n",
    "# Commiting and closing the connection from the database.\n",
    "database.commit()\n",
    "database.close()\n",
    "\n",
    "# Displaying the dataframe.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q3 Difference between loc and iloc.</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "# loc\n",
    "- Selects data based on labels (row and/or column names) in the DataFrame's index or columns.\n",
    "Useful when you know the specific labels of the data you want to access.\n",
    "# iloc\n",
    "- Selects data based on integer positions within the DataFrame.\n",
    "Useful when you know the exact numerical positions of the rows and columns you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q4. What is the difference between supervised and unsupervised learning?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <th>Aspect</th>\n",
    "    <th>Supervised Learning</th>\n",
    "    <th>Unsupervised Learning</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Definition</td>\n",
    "    <td>Learning from labeled data, where the algorithm is trained on a dataset that includes input-output pairs.</td>\n",
    "    <td>Learning from unlabeled data, where the algorithm tries to identify patterns and relationships within the data.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Objective</td>\n",
    "    <td>To predict outcomes for new data based on learned relationships.</td>\n",
    "    <td>To find hidden structures or patterns in the data.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Data</td>\n",
    "    <td>Requires labeled data with known outputs.</td>\n",
    "    <td>Works with unlabeled data with unknown outputs.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Techniques</td>\n",
    "    <td>Regression, Classification</td>\n",
    "    <td>Clustering, Association</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Examples</td>\n",
    "    <td>Spam detection, Predicting house prices</td>\n",
    "    <td>Market segmentation, Anomaly detection</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Algorithms</td>\n",
    "    <td>Linear Regression, Decision Trees, Support Vector Machines</td>\n",
    "    <td>K-Means Clustering, Hierarchical Clustering, Apriori Algorithm</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Evaluation</td>\n",
    "    <td>Accuracy, Precision, Recall, F1-Score</td>\n",
    "    <td>Silhouette Score, Davies-Bouldin Index</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q5. Explain the bias-variance tradeoff.</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "# bias-variance tradeoff\n",
    "The tradeoff is about finding a balance between bias and variance to minimize the total error.\n",
    "\n",
    "- Bias is a kind of error introduced by approximating a problem by a simple model. Means that the model is too simple and does not understand the complex pattern in the data. This will lead to underfitting of the model. And the data points are outoff the target.\n",
    "\n",
    "- Variance as we know the spreed of the data. And High variance means that the is too complex and capture the noise in the data while training. The model will perform too well on training data but not on test data this will lead to overfitting.\n",
    "\n",
    "# tradeoff's\n",
    "Imagine a bullseye target:\n",
    "\n",
    "1. High bias, High variance: Shots are consistently off-target and the spreed of data is also high.\n",
    "2. Low bias, Low variance: Shote are very close to target and the data spreed is low.\n",
    "3. Low bias, High variance: Means the shots are close to target but widely spreed.\n",
    "4. High bias, Low variance: Means the shots are off the target but grouped closely with low spreed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q6. what are precision and recall? How are they different form accuracy?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "# Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives\n",
    "\n",
    "# Recall\n",
    "Recall (also known as Sensitivity or True Positive Rate) is the ratio of correctly predicted positive observations to all the actual positives. \n",
    "\n",
    "# Accuracy\n",
    "Accuracy is the ratio of correctly predicted observations to the total observations.\n",
    "\n",
    "# Difference:\n",
    "\n",
    "- Precision focuses on the quality of positive predictions.\n",
    "- Recall focuses on the model's ability to capture all positive instances.\n",
    "- Accuracy provides an overall measure of the model's correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q7. What is overfitting and how can it be prevented?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "# Overfitting \n",
    "The condition when the model is perform too well on training data and give high accuracy, mean while perform too bad on test data and give low accuracy, this condition is called as overfitting. This is the common problem in machine learning.\n",
    "\n",
    "# How can it be prevented\n",
    "- Reducing the number of irrelevant or less important features.\n",
    "- Train with more data.\n",
    "- Handle outliers.\n",
    "- Use regularization like L1, L2.\n",
    "- Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q8. Explain the concept of cross-validation.</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "Cross-validation is a technique used to assess how a machine learning model will generalize to an independent dataset. It is primarily used to evaluate the performance of a model by partitioning the original dataset into a training set to train the model and a test set to evaluate it. This helps in ensuring that the model is not overfitting and can generalize well to new data.\n",
    "\n",
    "Why Cross-Validation?\n",
    "- Reliable Performance Estimates: Provides a more reliable measure of model performance compared to a simple train/test split.\n",
    "- Better Utilization of Data: Uses all data points for both training and testing, which is particularly useful when data is limited.\n",
    "- Model Selection and Hyperparameter Tuning: Helps in selecting the best model and tuning hyperparameters by comparing performance across different fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q9. What is the difference between a classification and a regression problem?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "<p><b>Classification:</b> When the output or target feature having categorical data (like identify the yes or no) and by using input features we have to predict these categories. This kind of problem called classification problem. There are two type of classification problems binary-class classification and multi-class classification.</p>\n",
    "\n",
    "<p><b>Regression:</b> In the regression problem the output feature is numerical and continuous in nature like prise of house. This problem statement can be simple linear regression and multi-linear regression.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q10. Explain the concept of ensemble learning.</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "Ensemble learning is a machine learning technique that combines multiple models, often called \"weak learners\" or \"base models,\" to create a more powerful predictive model. The main idea is that a group of models can often outperform any single model by leveraging their diverse strengths and compensating for their weaknesses.\n",
    "\n",
    "Types of Ensemble Learning Methods\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q11. What is gradient descent and how does it work?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning and statistical models. It is widely used for training models, especially in neural networks and linear regression. The goal is to find the parameters (weights) that minimize the cost function, which measures the error between the predicted values and the actual values.\n",
    "\n",
    "How Gradient Descent Works\n",
    "- Initialize Parameters: Start with initial values for the parameters (weights) of the model, often set to small random numbers or zeros.\n",
    "- Compute the Gradient: Calculate the gradient of the cost function with respect to each parameter. The gradient is a vector of partial derivatives that indicates the direction of the steepest ascent of the cost function.\n",
    "- Update Parameters: Adjust the parameters in the opposite direction of the gradient. The step size for each update is determined by a learning rate, a small positive value.\n",
    "- Repeat: Iterate over the steps of computing the gradient and updating the parameters until the cost function converges to a minimum, meaning the changes in the cost function value are sufficiently small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q12. Describe the difference between batch gradient descent and stochastic gradient descent.</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "Batch Gradient Descent:\n",
    "\n",
    "- Uses the entire dataset to compute the gradient of the cost function.\n",
    "- Can be computationally expensive for large datasets.\n",
    "- Provides a stable and accurate path towards the minimum but may be slow.\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "- Uses one training example at each iteration to compute the gradient.\n",
    "- Updates the parameters more frequently, leading to faster convergence.\n",
    "- Introduces more noise in the updates, which can help escape local minima but may cause the cost function to fluctuate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q13. What is curse of dimensionality in machine learning?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "The \"curse of dimensionality\" refers to various problems that arise when analyzing and organizing data in high-dimensional spaces. As the number of dimensions increases, the volume of the space increases exponentially, leading to several challenges in machine learning and data analysis.\n",
    "\n",
    "Challenges \n",
    "- Increased Computational Complexity\n",
    "- Increased Dimensionality\n",
    "- Overfitting\n",
    "- Increased Data Requirenments\n",
    "\n",
    "Solutions\n",
    "- Dimensionality Reduction\n",
    "- Feature Selection\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q14. Explain the difference between L1 and L2 regularization?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "L1 and L2 regularization are techniques used to prevent overfitting in machine learning models by adding a penalty to the loss function based on the magnitude of the model coefficients (weights).\n",
    "\n",
    "L1 Regularization (Lasso)\n",
    "- Definition: L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), adds a penalty equal to the absolute value of the magnitude of coefficients.\n",
    "\n",
    "- Example Use Case: Lasso is particularly useful when you believe only a few features are relevant and you want to select those features automatically.\n",
    "\n",
    "L2 Regularization (Ridge)\n",
    "- Definition: L2 regularization, also known as Ridge, adds a penalty equal to the square of the magnitude of coefficients.\n",
    "- Example Use Case: Ridge is useful when all features are believed to contribute to the target variable and you want to shrink the coefficients to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q15. What is a confusion matix and how it is used?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing the actual labels with the predicted labels. It provides a detailed breakdown of the classification results and helps in understanding the performance of a model beyond simple accuracy.\n",
    "\n",
    "Components of a Confusion Matrix\n",
    "For a binary classification problem, the confusion matrix is a 2x2 table that contains the following entries:\n",
    "\n",
    "- True Positive (TP): The number of correct positive predictions (actual positive and predicted positive).\n",
    "\n",
    "- True Negative (TN): The number of correct negative predictions (actual negative and predicted negative).\n",
    "\n",
    "- False Positive (FP): The number of incorrect positive predictions (actual negative but predicted positive). Also known as Type I error.\n",
    "\n",
    "- False Negative (FN): The number of incorrect negative predictions (actual positive but predicted negative). Also known as Type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q16. Define AUC-ROC curve?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "The AUC-ROC curve is a performance measurement for classification problems at various threshold settings. AUC stands for \"Area Under the Curve,\" while ROC stands for \"Receiver Operating Characteristic.\"\n",
    "\n",
    "ROC Curve\n",
    " - The ROC curve is a graphical representation that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It plots two parameters:\n",
    "\n",
    "- True Positive Rate (TPR), also known as Sensitivity or Recall: This is the proportion of actual positives that are correctly identified by the model.\n",
    "-  False Positive Rate (FPR): This is the proportion of actual negatives that are incorrectly identified by the model as positives\n",
    "\n",
    "AUC (Area Under the Curve)\n",
    "- AUC is the area under the ROC curve. It provides a single scalar value that summarizes the performance of the classifier across all threshold values.\n",
    "- The value of AUC ranges between 0 and 1. A model with a higher AUC is generally considered better because it indicates that the model has a good measure of separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q17. Explain the k-nearest neighbors algorithm?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "The k-nearest neighbors (K-NN) algorithm is a simple, and widely used machine learning algorithm for classification and regression tasks. \n",
    "\n",
    "1. Training Phase:\n",
    "\n",
    "- The K-NN algorithm doesn't explicitly train a model. Instead, it stores the entire training dataset.\n",
    "- No actual training process or parameter estimation occurs, making it a type of instance-based learning or lazy learning.\n",
    "\n",
    "2. Prediction Phase:\n",
    "\n",
    "- When making a prediction for a new instance (test point), the algorithm calculates the distance between this test point and all the instances in the training dataset.\n",
    "- The most common distance metric used is Euclidean distance, but other metrics like Manhattan distance or Minkowski distance can also be used.\n",
    "- The algorithm then selects the k closest training instances (neighbors) to the test point. The value of k is a user-defined parameter.\n",
    "\n",
    "# Advantages\n",
    "- Simple and Intuitive: Easy to understand and implement.\n",
    "- Non-parametric: No assumptions about the underlying data distribution.\n",
    "- Versatile: Can be used for both classification and regression problems.\n",
    "# Disadvantages\n",
    "- Computationally Intensive: Prediction can be slow, especially with large datasets.\n",
    "- Memory Intensive: Requires storing the entire training dataset.\n",
    "- Sensitive to Irrelevant Features: All features contribute equally to the distance computation unless carefully preprocessed.\n",
    "- Curse of Dimensionality: Performance can degrade with high-dimensional data, as distances become less meaningful.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q18. Explain the basic concept of a Support Vector Machine (SVM).</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful and versatile supervised learning models used for classification and regression tasks. The fundamental concept of SVMs is to find the best possible boundary that separates different classes in the feature space. The primary goal of an SVM is to find the optimal hyperplane that best separates different classes in the feature space. In a two-dimensional space, this hyperplane is a line, while in higher dimensions, it becomes a flat affine subspace.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q19. How does the kernel tick work Support Vector Machine (SVM)?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "\n",
    "The kernel trick in Support Vector Machines (SVM) is a powerful technique that allows the algorithm to handle non-linearly separable data by transforming the original feature space into a higher-dimensional space where a linear separation is possible. \n",
    "\n",
    "1. Non-Linearly Separable Data: In the original feature space, the data points of different classes might be intermixed in such a way that no straight line can separate them.\n",
    "\n",
    "2. Transformation: To address this, the kernel trick applies a mathematical function (the kernel function) to transform the data points into a higher-dimensional space.\n",
    "\n",
    "3. Kernel Function: The kernel function computes the inner product between the images of all pairs of data points in the higher-dimensional space.\n",
    "\n",
    "4. Common Kernels:\n",
    "- Linear Kernel\n",
    "- Polynomial Kernel\n",
    "- Radial Basis Function (RBF) Kernel\n",
    "- Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q20. What are the different types of kernels used in SVM and when you use each?</h3>\n",
    "<p><b>Answer</b></p>\n",
    "In Support Vector Machines (SVM), different types of kernels can be used to transform the data into higher-dimensional spaces where it may be easier to find a linear separation between classes. \n",
    "\n",
    "1. Linear Kernel\n",
    "- Function: K(Xi, Xj)= Xi*Xj\n",
    "2. Polynomial Kernel \n",
    "- Finction: K(Xi, Xj)= (Xi*Xj + c)<sup>d</sup>\n",
    "- Parameters: c (a constant), d (degree of the polynomial)\n",
    "\n",
    "3. Radial Basis Function (RBF) Kernel\n",
    "- Function: K(Xi, Xj)= exp(-𝛾 * square of (||Xi-Xj||))\n",
    "- Parameters: 𝛾 (controls the width of the Gaussian function)\n",
    "\n",
    "4. Sigmoid Kernel\n",
    "- Function: K(Xi, Xj)= tanh(αXi*Xj+c)\n",
    "- Parameters: α (scale parameter), c (offset parameter)\n",
    "\n",
    "When to Use Each Kernel\n",
    "- Linear Kernel: Use when the data is linearly separable or nearly linearly separable, and the number of features is high.\n",
    "- Polynomial Kernel: Use when you need to capture interactions between features and the decision boundary is polynomial in nature.\n",
    "- RBF Kernel: Use when the decision boundary is complex, and you have little prior knowledge about the data distribution. It is a good default choice for most problems.\n",
    "- Sigmoid Kernel: Use when the problem is similar to those solved by neural networks, though it's less common in practice compared to the other kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
